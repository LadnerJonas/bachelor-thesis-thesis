# used references
# shuffle operator
@article{RDMA-aware-Data-Shuffling-Operator,
  title        = {Design and Evaluation of an RDMA-aware Data Shuffling Operator for Parallel Database Systems},
  author       = {Liu, Feilong and Yin, Lingyan and Blanas, Spyros},
  year         = 2019,
  month        = dec,
  journal      = {ACM Trans. Database Syst.},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  volume       = 44,
  number       = 4,
  doi          = {10.1145/3360900},
  issn         = {0362-5915},
  url          = {https://doi.org/10.1145/3360900},
  issue_date   = {December 2019},
  abstract     = {The commoditization of high-performance networking has sparked research interest in the RDMA capability of this hardware. One-sided RDMA primitives, in particular, have generated substantial excitement due to the ability to directly access remote memory from within an application without involving the TCP/IP stack or the remote CPU. This article considers how to leverage RDMA to improve the analytical performance of parallel database systems. To shuffle data efficiently using RDMA, one needs to consider a complex design space that includes (1) the number of open connections, (2) the contention for the shared network interface, (3) the RDMA transport function, and (4) how much memory should be reserved to exchange data between nodes during query processing. We contribute eight designs that capture salient tradeoffs in this design space as well as an adaptive algorithm to dynamically manage RDMA-registered memory. We comprehensively evaluate how transport-layer decisions impact the query performance of a database system for different generations of InfiniBand. We find that a shuffling operator that uses the RDMA Send/Receive transport function over the Unreliable Datagram transport service can transmit data up to 4\texttimes{} faster than an RDMA-capable MPI implementation in a 16-node cluster. The response time of TPC-H queries improves by as much as 2\texttimes{}.},
  articleno    = 17,
  numpages     = 45,
  keywords     = {parallel database systems, RDMA, Data shuffling}
}
# parallel/distributed db
@article{parallel-programming-assessment-for-stream-processing-applications,
  title        = {A parallel programming assessment for stream processing applications on multi-core systems},
  author       = {Gabriella Andrade and Dalvan Griebler and Rodrigo Santos and Luiz Gustavo Fernandes},
  year         = 2023,
  journal      = {Computer Standards \& Interfaces},
  volume       = 84,
  pages        = 103691,
  doi          = {https://doi.org/10.1016/j.csi.2022.103691},
  issn         = {0920-5489},
  url          = {https://www.sciencedirect.com/science/article/pii/S0920548922000587},
  keywords     = {Parallel software, Parallel computing systems, Programming productivity, Programming effort, Stream parallelism, Programming usability},
  abstract     = {Multi-core systems are any computing device nowadays and stream processing applications are becoming recurrent workloads, demanding parallelism to achieve the desired quality of service. As soon as data, tasks, or requests arrive, they must be computed, analyzed, or processed. Since building such applications is not a trivial task, the software industry must adopt parallel APIs (Application Programming Interfaces) that simplify the exploitation of parallelism in hardware for accelerating time-to-market. In the last years, research efforts in academia and industry provided a set of parallel APIs, increasing productivity to software developers. However, a few studies are seeking to prove the usability of these interfaces. In this work, we aim to present a parallel programming assessment regarding the usability of parallel API for expressing parallelism on the stream processing application domain and multi-core systems. To this end, we conducted an empirical study with beginners in parallel application development. The study covered three parallel APIs, reporting several quantitative and qualitative indicators involving developers. Our contribution also comprises a parallel programming assessment methodology, which can be replicated in future assessments. This study revealed important insights such as recurrent compile-time and programming logic errors performed by beginners in parallel programming, as well as the programming effort, challenges, and learning curve. Moreover, we collected the participants’ opinions about their experience in this study to understand deeply the results achieved.}
}
@inproceedings{Real-Time-Processing-of-big-data-streams,
  title        = {Real-Time Processing of Big Data Streams: Lifecycle, Tools, Tasks, and Challenges},
  author       = {Gürcan, Fatih and Berigel, Muhammet},
  year         = 2018,
  month        = {Oct},
  booktitle    = {2018 2nd International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)},
  volume       = {},
  number       = {},
  pages        = {1--6},
  doi          = {10.1109/ISMSIT.2018.8567061},
  issn         = {},
  abstract     = {In today's technological environments, the vast majority of big data-driven applications and solutions are based on real-time processing of streaming data. The real-time processing and analytics of big data streams play a crucial role in the development of big-data driven applications and solutions. From this perspective, this paper defines a lifecycle for the real-time big data processing. It describes existing tools, tasks, and frameworks by associating them with the phases of the lifecycle, which include data ingestion, data storage, stream processing, analytical data store, and analysis and reporting. The paper also investigates the real-time big data processing tools consisting of Flume, Kafka, Nifi, Storm, Spark Streaming, S4, Flink, Samza, Hbase, Hive, Cassandra, Splunk, and Sap Hana. As well as, it discusses the up-to-date challenges of the real-time big data processing such as “volume, variety and heterogeneity”, “data capture and storage”, “inconsistency and incompleteness”, “scalability”, “real-time processing”, “data visualization”, “skill requirements”, and “privacy and security”. This paper may provide valuable insights into the understanding of the lifecycle, related tools and tasks, and challenges of real-time big data processing.},
  keywords     = {Real-time systems;Big Data;Tools;Task analysis;Sparks;Distributed databases;Batch production systems;Big data streams;real-time big data processing;lifecycle;tools;tasks;challenges}
}
@inproceedings{join-processing-in-parallel-db,
  title        = {From Theory to Practice: Efficient Join Query Evaluation in a Parallel Database System},
  author       = {Chu, Shumo and Balazinska, Magdalena and Suciu, Dan},
  year         = 2015,
  booktitle    = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  location     = {Melbourne, Victoria, Australia},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {SIGMOD '15},
  pages        = {63–78},
  doi          = {10.1145/2723372.2750545},
  isbn         = 9781450327589,
  url          = {https://doi.org/10.1145/2723372.2750545},
  abstract     = {Big data analytics often requires processing complex queries using massive parallelism, where the main performance metrics is the communication cost incurred during data reshuffling. In this paper, we describe a system that can compute efficiently complex join queries, including queries with cyclic joins, on a massively parallel architecture. We build on two independent lines of work for multi-join query evaluation: a communication-optimal algorithm for distributed evaluation, and a worst-case optimal algorithm for sequential evaluation. We evaluate these algorithms together, then describe novel, practical optimizations for both algorithms.},
  numpages     = 16,
  keywords     = {parallel database system, join query evaluation}
}
@inproceedings{locality-aware-partitioning-in-parallel-db,
  title        = {Locality-aware Partitioning in Parallel Database Systems},
  author       = {Zamanian, Erfan and Binnig, Carsten and Salama, Abdallah},
  year         = 2015,
  booktitle    = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  location     = {Melbourne, Victoria, Australia},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {SIGMOD '15},
  pages        = {17–30},
  doi          = {10.1145/2723372.2723718},
  isbn         = 9781450327589,
  url          = {https://doi.org/10.1145/2723372.2723718},
  abstract     = {Parallel database systems horizontally partition large amounts of structured data in order to provide parallel data processing capabilities for analytical workloads in shared-nothing clusters. One major challenge when horizontally partitioning large amounts of data is to reduce the network costs for a given workload and a database schema. A common technique to reduce the network costs in parallel database systems is to co-partition tables on their join key in order to avoid expensive remote join operations. However, existing partitioning schemes are limited in that resptect since only subsets of tables in complex schemata sharing the same join key can be co-partitioned unless tables are fully replicated.In this paper we present a novel partitioning scheme called predicate-based reference partition (or PREF for short) that allows to co-partition sets of tables based on given join predicates. Moreover, based on PREF, we present two automatic partitioning design algorithms to maximize data-locality. One algorithm only needs the schema and data whereas the other algorithm additionally takes the workload as input. In our experiments we show that our automated design algorithms can partition database schemata of different complexity and thus help to effectively reduce the runtime of queries under a given workload when compared to existing partitioning approaches.},
  numpages     = 14,
  keywords     = {partitioning schemes}
}
# slotted pages
@article{Data-page-layouts-for-relational-databases,
  title        = {Data page layouts for relational databases on deep memory hierarchies},
  author       = {Ailamaki, Anastassia and DeWitt, David J. and Hill, Mark D.},
  year         = 2002,
  month        = nov,
  journal      = {The VLDB Journal},
  publisher    = {Springer-Verlag},
  address      = {Berlin, Heidelberg},
  volume       = 11,
  number       = 3,
  pages        = {198–215},
  doi          = {10.1007/s00778-002-0074-9},
  issn         = {1066-8888},
  url          = {https://doi.org/10.1007/s00778-002-0074-9},
  issue_date   = {November 2002},
  abstract     = {Relational database systems have traditionally optimized for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results (which were obtained without using any indices on the participating relations), when compared to NSM: (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75\% of NSM's stall time due to data cache accesses; (b) range selection queries and updates on memory-resident relations execute 1725\% faster; and (c) TPC-H queries involving I/O execute 1148\% faster. Finally, we show that PAX performs well across different memory system designs.},
  numpages     = 18,
  keywords     = {Cache-conscious database systems, Disk page layout, Relational data placement}
}
# unused references
@article{radix-partitioning-case,
  title        = {On the surprising difficulty of simple things: the case of radix partitioning},
  author       = {Schuhknecht, Felix Martin and Khanchandani, Pankaj and Dittrich, Jens},
  year         = 2015,
  month        = may,
  journal      = {Proc. VLDB Endow.},
  publisher    = {VLDB Endowment},
  volume       = 8,
  number       = 9,
  pages        = {934–937},
  doi          = {10.14778/2777598.2777602},
  issn         = {2150-8097},
  url          = {https://doi.org/10.14778/2777598.2777602},
  issue_date   = {May 2015},
  abstract     = {Partitioning a dataset into ranges is a task that is common in various applications such as sorting [1,6,7,8,9] and hashing [3] which are in turn building blocks for almost any type of query processing. Especially radix-based partitioning is very popular due to its simplicity and high performance over comparison-based versions [6].},
  numpages     = 4
}
@inproceedings{main-memory-partitioning,
  title        = {A comprehensive study of main-memory partitioning and its application to large-scale comparison- and radix-sort},
  author       = {Polychroniou, Orestis and Ross, Kenneth A.},
  year         = 2014,
  booktitle    = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
  location     = {Snowbird, Utah, USA},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {SIGMOD '14},
  pages        = {755–766},
  doi          = {10.1145/2588555.2610522},
  isbn         = 9781450323765,
  url          = {https://doi.org/10.1145/2588555.2610522},
  abstract     = {Analytical database systems can achieve high throughput main-memory query execution by being aware of the dynamics of highly-parallel modern hardware. Such systems rely on partitioning to cluster or divide data into smaller pieces and thus achieve better parallelism and memory locality. This paper considers a comprehensive collection of variants of main-memory partitioning tuned for various layers of the memory hierarchy. We revisit the pitfalls of in-cache partitioning, and utilizing the crucial performance factors, we introduce new variants for partitioning out-of-cache. Besides non-in-place variants where linear extra space is used, we introduce large-scale in-place variants, and propose NUMA-aware partitioning that guarantees locality on multiple processors. Also, we make range partitioning comparably fast with hash or radix, by designing a novel cache-resident index to compute ranges. All variants are combined to build three NUMA-aware sorting algorithms: a stable LSB radix-sort; an in-place MSB radix-sort using different variants across memory layers; and a comparison-sort utilizing wide-fanout range partitioning and SIMD-optimal in-cache sorting. To the best of our knowledge, all three are the fastest to date on billion-scale inputs for both dense and sparse key domains. As shown for sorting, our work can serve as a tool for building other operations (e.g., join, aggregation) by combining the most suitable variants that best meet the design goals.},
  numpages     = 12,
  keywords     = {NUMA, SIMD, multi-core, parallelism, partitioning, sorting}
}
@inproceedings{data-partitioning-in-memory-systems,
  title        = {Data Partitioning for In-Memory Systems: Myths, Challenges, and Opportunities},
  author       = {Zhang, Zuyu and Deshmukh, Harshad and Patel, Jignesh M.},
  year         = 2019,
  booktitle    = {Proceedings of the 9th Biennial Conference on Innovative Data Systems Research (CIDR 2019)},
  publisher    = {CIDR},
  address      = {Chaminade, CA, USA},
  url          = {https://www.cidrdb.org/cidr2019/papers/p133-zhang-cidr19.pdf}
}
@inproceedings{main-memory-partitioning-study,
  title        = {A comprehensive study of main-memory partitioning and its application to large-scale comparison- and radix-sort},
  author       = {Polychroniou, Orestis and Ross, Kenneth A.},
  year         = 2014,
  booktitle    = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
  location     = {Snowbird, Utah, USA},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {SIGMOD '14},
  pages        = {755–766},
  doi          = {10.1145/2588555.2610522},
  isbn         = 9781450323765,
  url          = {https://doi.org/10.1145/2588555.2610522},
  abstract     = {Analytical database systems can achieve high throughput main-memory query execution by being aware of the dynamics of highly-parallel modern hardware. Such systems rely on partitioning to cluster or divide data into smaller pieces and thus achieve better parallelism and memory locality. This paper considers a comprehensive collection of variants of main-memory partitioning tuned for various layers of the memory hierarchy. We revisit the pitfalls of in-cache partitioning, and utilizing the crucial performance factors, we introduce new variants for partitioning out-of-cache. Besides non-in-place variants where linear extra space is used, we introduce large-scale in-place variants, and propose NUMA-aware partitioning that guarantees locality on multiple processors. Also, we make range partitioning comparably fast with hash or radix, by designing a novel cache-resident index to compute ranges. All variants are combined to build three NUMA-aware sorting algorithms: a stable LSB radix-sort; an in-place MSB radix-sort using different variants across memory layers; and a comparison-sort utilizing wide-fanout range partitioning and SIMD-optimal in-cache sorting. To the best of our knowledge, all three are the fastest to date on billion-scale inputs for both dense and sparse key domains. As shown for sorting, our work can serve as a tool for building other operations (e.g., join, aggregation) by combining the most suitable variants that best meet the design goals.},
  numpages     = 12,
  keywords     = {NUMA, SIMD, multi-core, parallelism, partitioning, sorting}
}
@inproceedings{Micro-Partitioning,
  title        = {Micro Partitioning: Friendly to the Hardware and the Developer},
  author       = {M\"{u}hlig, Jan and Teubner, Jens},
  year         = 2023,
  booktitle    = {Proceedings of the 19th International Workshop on Data Management on New Hardware},
  location     = {Seattle, WA, USA},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {DaMoN '23},
  pages        = {27–34},
  doi          = {10.1145/3592980.3595310},
  isbn         = 9798400701917,
  url          = {https://doi.org/10.1145/3592980.3595310},
  abstract     = {Modern hardware’s complexity has made studying hardware-conscious algorithms a relevant topic for many years. Partitioning algorithms, for instance, break data into bits that fit into fast CPU caches. Unfortunately, they are often challenging to design, develop, and maintain. While hardware-oblivious algorithms are easier to build, they may perform poorly when hardware or data deviate from expectations. In this paper, we introduce micro partitioning, which enhances the partitioning problem in a way that outperforms state-of-the-art solutions while being hardware-agnostic. By storing tuples in a tight address space, micro partitioning creates an access pattern that is friendly to both caches and translation lookaside buffers. We also show how micro partitioning interacts with task-based execution strategies in a symbiotic way, making micro partitioning intuitive to express for developers.},
  numpages     = 8
}
