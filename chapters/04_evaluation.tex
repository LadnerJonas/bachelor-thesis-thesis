% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Evaluation}\label{chapter:evaluation}\acresetall
In this chapter, we explain our benchmarks and evaluate the implementation of the shuffle operator.
Furthermore, we discuss best-case upper bounds to put our solution's performance in perspective.
\section{Benchmarking Setup}\label{section-benchmarking-setup}
We executed the following benchmarks on a 125 GiB (DDR4-2666) x86-64 machine with an Intel(R) Core(TM) i9-7900X CPU.
The machine runs Ubuntu 24.04.1 LTS with the Linux 6.5 kernel.
The C++23 code was compiled using GCC 13.3.0 with the optimization flags \texttt{-O2 -march=native -flto}.

\section{Benchmarks}
We simulate the real-world usage of the shuffle operator using a parallel tuple generation and shuffle, followed by a final tuple count check.

\subsection{Benchmark Overview}
\subsubsection{Tuple Generation}
This phase initializes a Mersenne Twister pseudo-random 64-bit number generator with a true-random seed.
The generator creates batches of tuples based on the total requested batch size without generating each tuple individually.

For each tuple batch, we allocate a new memory block, which the implementations then process and store onto slotted pages.
Similarly, real-world systems send tuples via slotted pages, which reside in different memory locations in the receiver system.

Each implementation follows the same tuple generation process, varying only in the size of the generated batches.
\subsubsection{Tuple Shuffle}
In this step, we simulate the real-world usage of the shuffle operator.
The operator processes an input stream of tuple batches and produces a partitioned output stream in the form of slotted pages.

To ensure a fair comparison between implementations, each implementation must generate a partitioned output stream with the following properties: For each partition, all slotted pages must be fully utilized except for the last one, and each slotted page must store its tuples as a contiguous block starting from the first slot.
These conditions ensure that all implementations produce the same result, with potential differences only in ordering tuples across slotted pages.
\subsubsection{Final tuple count check}
After the implementation has processed all tuples, we scan over all slotted pages and sum up the tuple count.
This way, we ensure that all tuples are written in their final location.

\subsection{Shuffle Operator Benchmark}
We executed the following benchmarks on the system described in Section \ref{section-benchmarking-setup}.
During execution, we recorded Linux perf counters, and these measurements form the foundation of the following evaluation.

We use three tuple sizes (4B, 16B, 100B) to simulate shuffle operator usage.
Since the first 4 bytes of each tuple serve as the partitioning key, we store these 4 bytes in the slot.
The remaining bytes are stored as variable-sized data in the data section of the slotted page.
In the 4-byte case, this storage approach uses only the slot section, as no additional data needs to be stored.
In contrast, the 16B and 100B tuples split into a 4-byte key part stored in the slot and the remaining 12B and 96B, which reside in the data section.

Furthermore, we recorded two best-case references for the shuffle implementations to put the results into perspective.
The first reference simulates tuple writes onto unsynchronized slotted pages, while the second uses shared and thus synchronized slotted pages.
Both implementations leverage \acfp{SMB} to achieve a best-case upper bound performance.
The best-case references always fully fill the \ac{SMB} before storing it in the $n$ partition.
After this batched write-out, the \ac{SMB} resets, and the next tuples are stored in the $n+1$ partition.
After writing to the last partition, the process restarts from the first partition.
Since our tuple generator creates uniformly distributed keys across all partitions, these references represent best-case upper bounds that are rarely achievable.

In the following benchmarks, we use a constant \ac{SMB} size per implementation, independent of the thread count.
For example, if an implementation uses a 2 MiB \ac{SMB} and 8 threads, each thread operates with $250 \textrm{\;KiB}$ of \ac{SMB}:
\begin{equation}
  \frac{\textrm{SMB Size}} {\textrm{Thread count}} = \frac{2 \textrm{\;MiB}}{8} = 250 \textrm{\;KiB}
\end{equation}
Since \ac{SMB} size varies between different thread configurations within the same implementation, small performance jumps occasionally appear in the benchmark plots.
These jumps occur when the \ac{SMB} size aligns with L1/L2 cache lines.

\subsubsection{Performance with few Partitions}
The following figures illustrate the processed tuples per second of each implementation as the thread count increases.
As the benchmarking machine has only 10 physical cores, it is expected that only minimal performance improvements can be achieved beyond these 10 physical cores.
The two best-case references perform very similarly with a few threads, but as the thread count increases, the synchronization causes contention, and thus, the performance gain per thread decreases.

\begin{figure}[h]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/Tuples_per_Second-Tuple16-Partitions2.pgf}}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/Tuples_per_Second-Tuple16-Partitions4.pgf}}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/legend.pgf}}
  \end{subfigure}
  \caption[Shuffle Benchmark Plots for Tuple of 16B with 2 and 4 Partitions]{Benchmark Plots for Tuple of 16B with 2 and 4 Partitions}
  \label{plot-shuffle-16B-2-4}
\end{figure}
In figure \ref{plot-shuffle-16B-2-4}, we can see that the LocalPagesAndMerge-implementation processes by far the most tuples per second.
This approach has the highest throughput, using 20\% fewer cycles, and can execute 25\% more \ac{IPC}.
It even exceeds the synchronized best-case reference, as the LocalPagesAndMerge-approach avoids synchronizing during write-out at the cost of a small sort-and-merge phase at the end.

In the four partition plots, we can already see that the performance gap of the LocalPagesAndMerge implementation decreases compared to the other implementations.
This is because the other approaches, which suffer from contention due to synchronization, benefit from increasing partition counts as the workload becomes more evenly distributed.

After the LocalPagesAndMerge-implementation, there is a group of three \acf{SMB}-based implementations: Smb, SmbBatched, and SmbLockFreeBatched.
These three implementations struggle to scale with increasing thread count in the two partition case, as having to write all the tuples on two shared slotted pages causes much contention.
Compared to the LocalPagesAndMerge approach, they use slightly more cycles and have fewer \ac{IPC}.
However, it can already be seen that with an increasing number of partitions, these three implementations can scale with increasing thread count.

The next group contains three implementations: \acf{CMP} with Processing Units, Hybrid, and Radix.
The leader of this group is the Hybrid implementation, which, especially in the two partition case, scales better than the other two implementations.
The Hybrid implementation can process more tuples there, as it does not need to materialize them fully.
Furthermore, the slotted pages are allocated when needed, and each thread can request memory locations without waiting for other threads to hand in their histogram.
The \ac{CMP} with Processing Units implementation performs better with increasing threads, as each processing unit contains one thread that only generates new tuples without partitioning them.
This processing unit's other threads can then process these generated tuples.

The remaining \ac{CMP}, OnDemand, and SmbLockFree implementations do not scale with increasing threads.
As explained in Section \ref{section-CMP-with_exclusive-partition-ranges}, the \ac{CMP} with exclusive partition ranges efficiency decreases per thread with an increase in the number of threads.
Furthermore, there are at most as many exclusive partition ranges as partitions.
Those additional threads cannot be used when there are more threads than exclusive partition ranges.
For example, when using only two partitions, there are only two exclusive partition ranges.
The remaining threads are not used because we can only assign those two partition ranges to exactly two threads.
The OnDemand approach does not use a \ac{SMB} and has to immediately write each tuple to its final slotted page.
As all slotted pages are shared, this causes a lot of contention.
Similarly, the non-batched lock-free \ac{SMB} implementation uses two busy-waiting loops that aim to update the tuple count of a slotted page using a compare-and-exchange operation.
Thus, this approach has a significantly higher number of branch misses (1-3x), instructions (1-4x), and L1-cache misses (1-2x) in comparison with the other \ac{SMB} implementations.
\begin{figure}[h]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/Tuples_per_Second-Tuple16-Partitions8.pgf}}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/Tuples_per_Second-Tuple16-Partitions16.pgf}}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/legend.pgf}}
  \end{subfigure}
  \caption[Shuffle Benchmark Plots for Tuple of 16B with 8 and 16 Partitions]{Benchmark Plots for Tuple of 16B with 8 and 16 Partitions}
  \label{plot-shuffle-16B-8-16}
\end{figure}

In figure \ref{plot-shuffle-16B-8-16}, which uses 8 and 16 partitions, we can see that the performance gap between the LocalPagesAndMerge and the other implementations shrinks with increasing partitions.
Also, the previous groups of implementations with similar performance characteristics remain.

The LocalPagesAndMerge implementation does not significantly increase its throughput, as with a higher number of partitions, there also must be more thread-local slotted pages per thread.
This causes more heap allocations, and the sort-and-merge phase is getting more expensive.
Furthermore, the \ac{LLC} misses and \ac{TLB} misses increase as more memory locations are accessed.

The previous group of SMB implementations (Smb, SmbBatched, and SmbLockFreeBatched) can now process very similar amounts of tuples per second as the LocalPagesAndMerge approach.
The synchronization contention decreases with increasing partitions, as the load on locks or atomics is now spread up between more slotted pages and thus across more locks or atomics.

The next group contains the following four implementations: CmpProcessingUnits, Hybrid, Radix, and SmbLockFree.
The leading implementation is the Radix approach, which performs better than the Hybrid implementation.
In comparison, the Radix implementations have higher L1-cache/LLC misses but have fewer branch- and TLB misses.
This leads to overall better CPU usage with less time idling.
All implementations of this group have significantly higher L1-cache/LLC misses than the previous group.

The last group of implementations, which does not scale well, contains yet again: CmpExclusivePartitionRanges, OnDemand, and SmbLockFree.
Similar to the previous figure \ref{plot-shuffle-16B-2-4}, increasing the number of partitions does not solve the decreasing efficiency per added thread issue of the CmpExclusivePartitionRanges implementation.
Similarly, the number of logical cores is higher than the maximum number of partitions, which causes that a few threads do not get assigned a partition range und thus, cannot be used.
Also, the OnDemand approach struggles even more to write out the tuples, as the number of slotted pages increases with increasing partitions.
This causes even more L1-cache/LLC misses, as the \ac{CPU} cannot hold all these memory locations in caches.
Like the previous benchmark, the SmbLockFree approach still uses the two busy waiting loops to atomically get the write-out information for a given tuple batch.
With an increasing number of partitions, these atomic tuple-count values are less frequently requested by multiple threads.
Thus, with increasing partitions, this implementation starts to improve its scaling.
\subsubsection{Performance with many Partitions}
After comparing the different implementations on 2 - 16 partitions, we now evaluate the performance on 32 and 1024 partitions.
This high number of partitions creates new challenges to the problem, as memory consumption plays a big role in the performance.
For example, when using 1024 partitions and our 5 MiB slotted page size, the initial memory consumption is already at 5 GiB.
When thread-local slotted pages are used, this initialization of a slotted page per partition and thread already takes up 100 GiB on our 20 logical processors (10 physical cores + SMT) machine.

\begin{figure}[h]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/Tuples_per_Second-Tuple4-Partitions32.pgf}}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/Tuples_per_Second-Tuple4-Partitions1024.pgf}}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/legend.pgf}}
  \end{subfigure}
  \caption[Shuffle Benchmark Plots for Tuple of 4B with 32 and 1024 Partitions]{Benchmark Plots for Tuple of 4B with 32 and 1024 Partitions}
  \label{plot-shuffle-4B-32-1024}
\end{figure}

In figure \ref{plot-shuffle-4B-32-1024}, we plot the performance of our implementations when using 4-byte tuples and 32 / 1024 partitions.
In this 4-byte tuple case, we store the whole tuple in the slot section as the first 4 bytes construct the tuple key.

Like the few partition cases, the LocalPagesAndMerge-implementation processes the most tuples per second in the 32 partition case.
In contrast, in the 1024 partition case, we can see that the total amount of unnecessary heap allocations and non-shared memory accesses causes performance to degrade and lead to a negative scaling.

Like in the previous figures, most \ac{SMB}-based implementations (Smb, SmbBatched, and SmbLockFreeBatched) scale very well while still struggling to keep up with the LocalPagesAndMerge in the 32 partition case.
The two lock-based approaches outperform the lock-free implementation in the 32 partition case, as they have slightly fewer branch misses and an overall higher \ac{IPC}.
In contrast, the batched lock-free implementation processes more tuples in the 1024 partition case, as it has higher \ac{IPC} and less \ac{LLC} misses.

The next two best implementations in the 32 partition case are the Hybrid and Radix implementation.
They can process slightly fewer tuples when using 32 partitions, whereas, with 1024 partitions, the gap between this and the previous group gets huge.
When using the 32 partitions, the Radix approach performs better as it only has half as many branch misses than the Hybrid implementation.
In contrast, the Hybrid implementations have 20\% higher \ac{IPC} in the 1024 partition case and thus perform better there.

The remaining four implementations (CmpExclusivePartitionRanges, CmpProcessingUnits, OnDemand, SmbLockFree) struggle to scale with increasing threads when using 32 partitions.
The CmpExclusivePartitionRanges has yet again extremely high numbers of branch misses due to the partition range exclusive tuple processing.
In contrast, the CmpProcessingUnits does scale with the number of threads in the 1024 partition case, but still up to 100x more branch misses than the best-performing implementations.
The OnDemand approach does have slightly fewer branch misses than the previous two implementations.
However, due to the single tuple write-out, it has up to 100x more \ac{TLB} misses than the other implementations.
Lastly, the lock-free non-batched \ac{SMB} implementation also struggles to perform in the 32 partition case but does work surprisingly well with 1024 partitions.
The branch and LLC misses are up to 3x more frequent than the other \ac{SMB} implementations.
The busy-waiting atomic accesses on the tuple-count variable of each slotted page cause the increase branch and LLC misses.
\begin{figure}[h]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/Tuples_per_Second-Tuple16-Partitions32.pgf}}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/Tuples_per_Second-Tuple16-Partitions1024.pgf}}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/legend.pgf}}
  \end{subfigure}
  \caption[Shuffle Benchmark Plots for Tuple of 16B with 32 and 1024 Partitions]{Benchmark Plots for Tuple of 16B with 32 and 1024 Partitions}
  \label{plot-shuffle-16B-32-1024}
\end{figure}

In figure \ref{plot-shuffle-16B-32-1024}, we use tuples with a size of 16 bytes instead of 4 bytes like in the previous figure.
As we now use a tuple size greater than 4 bytes, we must also write the remaining bytes in the data section at the end of the slotted page.

Like the previous benchmark plots, the LocalPagesAndMerge implementation processes the most tuples per second in the 32 partition case.
In contrast, when using 1024 partitions, this approach struggles to scale at all.
Each added thread requires an initial allocation of 1024 slotted pages (5 GiB) and can then process tuples.
In the end, these additional slotted pages must be sorted and merged, significantly increasing the cost of this final phase.

The overall best-performing group is the four \ac{SMB} implementations.
Besides the slightly slower non-batched lock-free implementation, they can keep up with the LocalPagesAndMerge implementation in the 32 partition case.
When using 1024 partitions, these four implementations outperform all other implementations by processing around 50\% more tuples.
The difference between the groups is that the lock-free non-batched variant has more branch misses, whereas the batched lock-free variant has slightly fewer \ac{LLC} misses than the two lock-based approaches.

The next group of implementations is the following three that scale with increasing threads but not as well as the \ac{SMB} variants: CmpProcessingUnits, Hybrid, and Radix.
The CmpProcessingUnits approach has significantly higher cache misses due to its selective write-out, significantly slowing down this implementation.
The Hybrid variant performs slightly worse than the \ac{SMB} implementations due to slightly more \ac{LLC} misses.
Lastly, the Radix implementation struggles to scale in the 32 partition case but performs better when using 1024 partitions.
It has around 50\% more L1-cache /\ac{LLC} misses than the Hybrid approach, which slows down this approach.

In the non-scaling group, there are two implementations: CmpExclusivePartitionRanges and OnDemand.
Like the previous benchmarks, the CmpExclusivePartitionRanges per thread efficiency decreases due to the more selective tuple write-out.
The more selection write-out is caused by the increase of threads, which decreases the size of each exclusive partition range.
Thus, the likelihood of processing a tuple that belongs to a certain partition range decreases.
The OnDemand struggles to scale, as it does not use any form of buffering, which causes high contention on locks and more frequent accesses on slotted pages.

\begin{figure}[h]
  \centering
  \begin{subfigure}{.49\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/Tuples_per_Second-Tuple100-Partitions32.pgf}}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/Tuples_per_Second-Tuple100-Partitions1024.pgf}}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \resizebox{\linewidth}{!}{\input{figures/evaluation/legend.pgf}}
  \end{subfigure}
  \caption[Shuffle Benchmark Plots for Tuple of 100B with 32 and 1024 Partitions]{Benchmark Plots for Tuple of 100B with 32 and 1024 Partitions}
  \label{plot-shuffle-100B-32-1024}
\end{figure}

In figure \ref{plot-shuffle-100B-32-1024}, we use 100-byte tuples instead of 16 bytes.
Due to the bigger tuples, the cache lines and the \acp{SMB} can hold fewer tuples.
This makes the write-out process more expensive and cache misses more likely.

Unlike previous benchmarks, the LocalPagesAndMerge approach does not process the most tuples in the 32 partition case.
This time, the SmbBatched variant can outperform all other implementations.
Nonetheless, the LocalPagesAndMerge scales still very well with increasing threads in the 32 partition case but has up to 2x more \ac{TLB} misses than the SmbBatched variant.
Like in the previous benchmarks, the LocalPagesAndMerge approach struggles to perform when using 1024 partitions.
As each newly added thread requires the allocation of 1024 slotted pages (5 GiB), and the slotted page merge phase becomes more expensive, we observe that performance worsens as more threads are added.

The next group, which scales very well with increasing threads, are the four \ac{SMB} implementations: Smb, SmbBatched, SmbLockFree, and SmbLockFreeBatched.
Overall, all four implementations have similar perf measurements, just the SmbBatched has slightly fewer \ac{LLC} misses and branch misses.

Hybrid and CmpProcessingUnits form the next group, as they scale with increasing threads in both partition cases, but are significantly less performant than the previous \ac{SMB} group.
The Hybrid approach causes 3x more \ac{LLC} misses, whereas the CmpProcessingUnits has up to 20x more branch misses than the SmbBatched variant.
These two significantly increased performance counters are causing a substantial performance decrease.

The next group consists again of two members: OnDemand and Radix.
Both approaches cannot scale with more threads in the 32 partition case, but perform resonable when using 1024 partitions.
The OnDemand approach performs substantially better in the 1024 partition case, as the tuple write-out is more expensive in the 100-byte tuple case.
This more expensive write-out and the fact that we are using 1024 partitions reduces the contention on the shared slotted page locks significantly.
As we only hold locks during the fetching of the tuple index, the more expensive tuple write slows down the write enough so that the locks are rarely accessed simultaneously.
The performance of the Radix implementation does increase due to a similar load-balancing effect.
In the Radix implementation, every thread has to use its histogram to request write-out locations.
We use a lock for each partition to make the implementation more scalable.
We have to hold the lock during the allocation of a slotted page, as it will frequently be the case that the following thread wants to request a write-out location on this newly allocated page.
By increasing the partitions, these locks are load-balanced as each thread starts at a different start partition and then requests a memory location for each partition.

Like in the previous benchmarks, the CmpExclusivePartitionRanges again cannot scale up, as the size of the exclusive partition ranges decreases with increasing thread count.
This means that for a given thread, the likelihood that the next tuple must be placed in a partition of its assigned exclusive partition range decreases.
This causes vast numbers of branch misses, and as all threads have to process all tuples, this implementation struggles to scale.

\subsection{Peak Heap Memory Benchmark}
We used the heap profiler Massif within Valgrind 3.24 to measure the heap memory consumption of our implementations.
In our memory benchmark, we partition 67.2 Mio.
tuples of 16 bytes ($\approx$ 1 GiB) into 32 partitions.
Since the tuple keys are distributed uniformly, the implementations store the tuples on ten 5 MiB slotted pages per partition.
To highlight the impact of the benchmark parameters on the heap usage, we executed the benchmark using 40 threads.

\begin{figure}[h]
  \centering
  \resizebox{\linewidth}{!}{\input{figures/evaluation/memory_plot-16B-P32-Th40.pgf}}
  \caption[Peak Heap Memory Benchmark Plot]{Peak Heap Usage when using 32 Partitions, 40 Threads and 67.2 Mio.
    16B Tuples (1 GiB)} \label{plot-heap-16B-P32-Th40} \end{figure}
In figure \ref{plot-heap-16B-P32-Th40}, we visualized the peak heap usage of each implementation.
The partitioning of 67.2 Mio.
tuples requires 10 slotted pages per partition.
These 10 slotted pages per partition sum up to around 1.46 GiB of heap memory, which each implementation must use and acts as a lower bound in the plot.
To better visualize the peak memory consumption of each implementation, we subtract the memory needed for the minimum number of slotted pages and show the additionally used heap memory above the bar of each implementation.

All implementations, except LocalPagesAndMerge and Radix, rely on a few small heap allocations (vectors, SMBs).
The OnDemand implementation uses the least amount of heap, as it does not use any \ac{SMB}.
In the implementations that only use a few heap allocations, the two \ac{CMP} approaches use the most heap.
They both use \acp{SMB}, and their producer-consumer approach uses vectors.

The Radix implementation must fully materialize the 1 GiB of tuple data before writing tuples to the slotted pages.
This materialization allocation directly scales with the total size of the incoming tuples.
The Hybrid approach avoids this by only materializing small chunks and assigning them to threads.

The LocalPagesAndMerge approach uses the most heap memory due to the thread-local slotted pages.
As explained in the space complexity analysis in \ref{section-space-complexity}, we want to emphasize that this additional heap memory consumption does not scale when increasing the incoming tuple count.
Instead, it results from each partition's last, not fully used, thread-local page.
These pages must be merged, so we pass the minimal number of slotted pages to the next operator.
Since we can have at most one not fully used thread-local slotted page per partition and thread, the number of slotted pages can be upper bounded by a constant:

\begin{equation} c_\textrm{LPAM} \leq (\textrm{Partition count}) * (\textrm{Thread count}) * (\textrm{Size of a slotted page}) \end{equation}

This constant $c_\textrm{LPAM}$ must be considered when using the LocalPagesAndMerge approach, as it can be several GiB, as seen in figure \ref{plot-heap-16B-P32-Th40}.
\section{Comparison with Stream Processing Systems}
We compare our shuffle operator with the \texttt{keyBy} operator of the batch and streaming processing engine Apache Flink\cite{apache-flink}.
This \texttt{keyBy} operator partitions an incoming data stream into disjoint partitions based on a key.
We use the Java 19 Client and version 1.20 of Apache Flink in the following benchmark.
Furthermore, we run the Apache Flink cluster on a single machine but let it use all available cores.

Like our implementations, we generate tuples on demand for a data stream.
We apply the \texttt{keyBy} operator on this data stream by using the tuple key and our partition function (see Section \ref{sec-partitioning}).
Then, we collect the count of records per partition.
\begin{figure}[h]
  \centering
  \resizebox{.75\linewidth}{!}{\input{figures/evaluation/apache-flink-comparison.pgf}}
  \caption[Tuples per Second Comparison Plot with Apache Flink]{Tuples per Second Comparision when using 1024 Partitions}
  \label{plot-apache-flink-comparison}
\end{figure}

In figure \ref{plot-apache-flink-comparison}, we compare the throughput of Apache Flink to our best-performing implementation, SmbLockFreeBatched.
Across all tuple sizes, the SmbLockFreeBatched approach performs significantly faster and requires only one-third of memory in comparison with the Flink approach.

The results of this benchmark are unexpected.
The Apache Flink benchmark simplifies to a frequency map.
For each tuple, we apply the partitioning function and then increment the frequency of the corresponding partition by one.
These two operations allow for heavy parallelization using a concurrent hash map.
However, we must consider that Apache Flink is written in Java and Scala, whereas our implementation uses C++23 and compiles into optimized native machine code.
